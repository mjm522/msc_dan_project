#Apply Linear regression to this example 
#Multivariate or singular 
#Learn and apply example multivariate
'''
Install SCIPY library to make this code work
pip install scipy

LR tutorials
https://www.analyticsvidhya.com/blog/2015/10/regression-python-beginners/
http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html

'''

import numpy as np
import scipy.linalg as la
import matplotlib.pyplot as plt

#This makes the random numbers predictable
np.random.seed(0)

#This creates a graph which is larger for the viewer
#plt.figure(1, figsize=(15,15))
#plt.ion()
#plt.show(False)


#solve the problem of accounting for noise with linear regression
#Pass paramters A?,B?, u, x in for loop of execution

def linearRegression(q,u,dynamics):

    #x,y,dx,dy
    #print("x","y","dx","dy")

    #save data to a file but in real life 

    #print(q, u)
    A = dynamics.A
    B = dynamics.B
    u = 0
    m = 0
    x = 0

    #compute the next state of x
    #x_nxt = (A * x) + (B * u) #Add non linear term + no(x,u)
    #compute the plane that it will show the line of best fit with m and b
    #p = (m * x) + (B * u) #???? + c

    #add derivative of m and b to iteratvely update?
    #iterate over X and u, do I need to update A and B also from data?

    #this will return the margin of error 
    #sum (p) - (A*x[i] + B*u[i] - x_nxt)^2 
    
    #call the draw function to draw plotted points and maybe show the error if possible 



'''
This funciton implements a discretized lqr controller
it takes in system matrix A, the control matrix B
The state penalization Q, and the control penalization R
It formualates the ricatti equation and solves it
Don't understand the materials given in the doc folder
'''

def dlqr(A, B, Q, R):
    """Solve the discrete time lqr controller.
    
    x[k+1] = A x[k] + B u[k]
     
    cost = sum x[k].T*Q*x[k] + u[k].T*R*u[k]
    """
    #ref Bertsekas, p.151
 
    #first, try to solve the ricatti equation
    X = la.solve_discrete_are(A, B, Q, R)
     
    #compute the LQR gain
    K = la.pinv( np.dot( (np.dot(np.dot(B.T, X), B) + R) , np.dot(np.dot(B.T, X), A) ).T )

    eigVals, eigVecs = la.eig( A - np.dot(B,K) )
     
    return K, eigVals


'''
This is a class that encapsulates the dynamics of a point mass object.
It has the A matrix, B matrix 
It also has an option to compute the non_linear_term when a configuration is passed
the compute_nxt_state funciton computes the next state if the current state and control is passed
the state q = [x, y, dx, dy]
the control  u = [Fx, Fy]
'''

class Dynamics():

    def __init__(self, dt=0.01):
        '''
        the system matrix
        '''
        self.A = np.array([[1.,0.,dt, 0.],
                           [0.,1.,0., dt],
                           [0.,0.,1., 0.],
                           [0.,0.,0., 1.]])
        '''
        the control matrix
        '''
        self.B = np.array([[0., 0.],
                           [0., 0.],
                           [dt, 0.],
                           [0., dt]])

    
    def non_linear_term(self, q):
        #print(q)
        return q + np.random.randn(4)


    def compute_nxt_state(self, q, u, disturb=False):
        
        q = np.dot(self.A, q) + np.dot(self.B, u)

        if disturb:
            q += self.non_linear_term(q)

        return q



def compute_ctrl(q0, qf, dynamics, correction_dynamics=None):
    '''
    find a control command using the start, final and the dynamics
    at present it is not doing anything, just returning a random control input
    so initially the correction dyanmics is none and later it learns it
    The state penalization Q, and the control penalization R
    '''
    Q = np.array([[1.,0.,0.,0.],
                  [0.,1.,0.,0.],
                  [0.,0.,10.,0.],
                  [0.,0.,0.,10.]])
    R = np.eye(2)*0.06

    K, eigVals = dlqr(dynamics.A, dynamics.B, Q, R)

    if correction_dynamics is not None:
        '''
        add the corrected dyanmics to the computed dyanmics
        '''
        pass

   #print "U \n", np.dot(-K, (q0-qf))
    #print(np.dot(-K, (q0-qf)))

    return np.dot(-K, (q0-qf))


def collect_data_and_learn_correction(dynamics, total_data_points=100):
    
    for k in range(total_data_points):

        u = compute_ctrl(q, qf, dynamics)

        q_nxt = dynamics.compute_nxt_state(q=q, u=u, disturb=True)

        data.append(np.r_[q, u, q_nxt])
        q_array.append(q)
        u_array.append(u)
        q_nxt_array.append(q_nxt)

        q = q_nxt

    np.savetxt('q.txt', q_array)
    np.savetxt('q_nxt.txt', q_nxt_array)
    np.savetxt('u.txt', u_array)
    #np.savetxt('data.txt', data)

    '''
    implement some method that will use this data to learn the correction model
    for example, 
    '''

    return correction_dynamics


def visualize(start, goal, point_mass_trajectory, error_list):
    plt.clf()
    plt.subplot(221)
    plt.scatter(start[0], start[1], color='r')
    plt.scatter(goal[0],  goal[1],  color='g')

    plt.plot(point_mass_trajectory[0,:], point_mass_trajectory[1,:], color='b')
    plt.xlim([-0.1, 1.])
    plt.ylim([-0.1, 1.])
    plt.xlabel("X location")
    plt.ylabel("Y location")

    plt.subplot(222)
    plt.plot(error_list, color='m')
    plt.xlabel("time steps")
    plt.ylabel("error magnitude")

    plt.subplot(223)
    plt.plot(point_mass_trajectory[2,:], color='r')
    plt.xlabel("time steps")
    plt.ylabel("x velocity magnitude")

    plt.subplot(224)
    plt.plot(point_mass_trajectory[3,:], color='g')
    plt.xlabel("time steps")
    plt.ylabel("y velocity magnitude")

    plt.draw()
    plt.pause(0.0001)

#Multivariate linear regression for two parameters x and u, which provides x_nxt
#https://gist.github.com/marcelcaraciolo/b7aa1cf3c60dae85785b
from numpy import loadtxt, zeros, ones, array, linspace, logspace, mean, std, arange
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from pylab import plot, show, xlabel, ylabel
import csv

#Evaluate the linear regression

def feature_normalize(X):
    '''
    Returns a normalized version of X where
    the mean value of each feature is 0 and the standard deviation
    is 1. This is often a good preprocessing step to do when
    working with learning algorithms.
    '''
    mean_r = []
    std_r = []

    X_norm = X

    n_c = X.shape[1]
    for i in range(n_c):
        m = mean(X[:, i])
        #print(m, X)
        s = std(X[:, i])
        mean_r.append(m)
        std_r.append(s)
        X_norm[:, i] = (X_norm[:, i] - m) / s

    return X_norm, mean_r, std_r


def compute_cost(X, y, theta):
    '''
    Comput cost for linear regression
    '''
    #Number of training samples
    m = y.size

    predictions = X.dot(theta)


    sqErrors = (predictions - y)

    J = (1.0 / (2 * m)) * sqErrors.T.dot(sqErrors)

    return J


def gradient_descent(X, y, theta, alpha, num_iters):
    '''
    Performs gradient descent to learn theta
    by taking num_items gradient steps with learning
    rate alpha
    '''
    m = y.size
    J_history = zeros(shape=(num_iters, 1))

    for i in range(num_iters):

        predictions = X.dot(theta)

        theta_size = theta.size

        for it in range(theta_size):

            temp = X[:, it]
            temp.shape = (m, 1)

            errors_x1 = (predictions - y) * temp

            theta[it][0] = theta[it][0] - alpha * (1.0 / m) * errors_x1.sum()

        J_history[i, 0] = compute_cost(X, y, theta)

    return theta, J_history

#Load the dataset
data = loadtxt('ex1data2real.txt', delimiter=',')
#imperfect model data - 100 points which didn't reach target
q = [0.0, 0.02, 0.040000000000000001, 0.040000000000000001, 0.050000000000000003, 0.070000000000000007, 0.040000000000000001, 0.070000000000000007, 0.089999999999999997, 0.080000000000000002, 0.10000000000000001, 0.089999999999999997, 0.089999999999999997, 0.080000000000000002, 0.080000000000000002, 0.089999999999999997, 0.080000000000000002, 0.089999999999999997, 0.089999999999999997, 0.10000000000000001, 0.10000000000000001, 0.089999999999999997, 0.11, 0.10000000000000001, 0.11, 0.12, 0.14000000000000001, 0.13, 0.16, 0.14999999999999999, 0.17000000000000001, 0.17999999999999999, 0.17999999999999999, 0.19, 0.19, 0.20000000000000001, 0.19, 0.20999999999999999, 0.20999999999999999, 0.20000000000000001, 0.22, 0.22, 0.20999999999999999, 0.23000000000000001, 0.23999999999999999, 0.23999999999999999, 0.23999999999999999, 0.25, 0.25, 0.25, 0.26000000000000001, 0.26000000000000001, 0.27000000000000002, 0.28000000000000003, 0.29999999999999999, 0.29999999999999999, 0.29999999999999999, 0.28000000000000003, 0.28000000000000003, 0.28000000000000003, 0.29999999999999999, 0.29999999999999999, 0.29999999999999999, 0.29999999999999999, 0.29999999999999999, 0.29999999999999999, 0.28999999999999998, 0.28000000000000003, 0.28000000000000003, 0.29999999999999999, 0.31, 0.32000000000000001, 0.32000000000000001, 0.33000000000000002, 0.35999999999999999, 0.37, 0.35999999999999999, 0.37, 0.38, 0.37, 0.38, 0.37, 0.35999999999999999, 0.35999999999999999, 0.37, 0.35999999999999999, 0.37, 0.37, 0.38, 0.40000000000000002, 0.39000000000000001, 0.40000000000000002, 0.40000000000000002, 0.40000000000000002, 0.40999999999999998, 0.42999999999999999, 0.40999999999999998, 0.41999999999999998, 0.44]
u = [8.4406054485615716, 6.8769867199972614, 5.5536708232371819, 4.6562507213980977, 3.8079629210057298, 3.0754979102968827, 2.5646408145549535, 2.0279787713770832, 1.5926680006091369, 1.4206493768218167, 1.1789333906435373, 1.2922869196467426, 1.290711578479665, 1.2864531616569959, 1.0928981613442668, 0.99662403844088643, 0.9790676342009409, 1.0517279612709192, 0.79436128342461609, 0.52910036734775923, 0.51432227977578693, 0.39364822524959742, 0.051055078351792578, 0.13402484645272522, 0.022832977973822617, -0.025484080753804483, 0.063204996260521687, 0.15134250884566908, -0.28273403997002627, -0.17910345068317957, -0.31763602643534761, -0.35736459358466915, -0.22369970116075227, -0.12394824385115025, 0.0045643588923267803, -0.10953501769925289, -0.069965683397994236, -0.053842258496426834, 0.005940279657798397, 0.033573140580857877, 0.13164160998797467, -0.036705259110271102, 0.16282111502996627, -0.087370716694710457, 0.024877339391087092, 0.10915165371485104, 0.18830936272030271, 0.28053349341891537, 0.4543046011916605, 0.28163989565731457, -0.16168646046707325, -0.31413545670164794, -0.31495797464026853, -0.44517302747785709, -0.28780249327191515, -0.61989243441571662, -0.4269275791319318, -0.33306934437266372, -0.050607874389868267, 0.14359126412140888, -0.039548436145425851, -0.003266096196912541, -0.19073268881265815, -0.22562791018278552, -0.32817423891657826, -0.01498003945326307, 0.11708389439229372, 0.088509300961026008, 0.0052610720775982094, -0.0091457529239726588, 0.26656015713392861, 0.31848798473822937, 0.39889673909207701, 0.38189709862331678, 0.21226704821623799, 0.21715201554551186, 0.24490668622256145, 0.36437889972344439, 0.2753539495651926, 0.32764292569154935, 0.26852145114033404, 0.19835879130545803, 0.22810452683459553, 0.088888434623917434, 0.43579285708519799, 0.54078082468560251, 0.49361604715623353, 0.47595993287789168, 0.47464797846166784, 0.41811742919002981, 0.4982793437180611, 0.22099717185112552, 0.19768981246958436, 0.116052202351046, 0.02976592741040076, 0.0068704166644160446, 0.42316632116618375, 0.49146227655588753, 0.22719037820500071]
q_nxt = [0.017738397258087218, 0.037450920344606183, 0.038157778040805709, 0.04810695491200901, 0.065883477105623339, 0.043656550321307344, 0.069969193577968047, 0.08918404156236244, 0.084361246589076219, 0.10083975001768844, 0.094501254906972212, 0.093544381432469736, 0.081584733335749587, 0.080782265273934326, 0.085818233918437084, 0.083492401049278631, 0.089602076649564696, 0.085035323003118654, 0.10108096680618289, 0.097072913147125031, 0.090220808852760237, 0.11007203508927856, 0.1043322603005631, 0.11142350365527429, 0.1165630911303407, 0.1403044201667103, 0.13343909787522437, 0.15772750629535259, 0.15416257151391938, 0.16871554779380829, 0.17757217317986829, 0.17564358569924263, 0.18727089631925198, 0.18534597818460927, 0.19595750066957077, 0.18590766284675328, 0.21449961297531692, 0.20608722052125367, 0.20337112447352906, 0.21925478303007859, 0.21899005725339066, 0.21136444342158395, 0.22796189805806899, 0.24116021871738247, 0.23770184629390942, 0.23867575729329576, 0.24934239874691744, 0.24625064097165381, 0.25024206599342341, 0.26260597876465192, 0.26369829759347374, 0.27486514880420931, 0.28248953508954044, 0.2950520145824731, 0.29976242390802288, 0.29500844752046751, 0.28435961240374863, 0.28296048789301087, 0.28195698413414011, 0.3013645056659891, 0.29945388256605554, 0.30093877834403682, 0.29743194109847287, 0.30352835868438449, 0.3007684142217007, 0.29328340213807574, 0.27837507567226411, 0.27977229857193608, 0.30378111634295712, 0.31296568646539269, 0.3160373126819408, 0.32299991303883518, 0.33396629819466428, 0.36109996338582628, 0.36616995608423347, 0.35720684088844767, 0.36786111438474844, 0.37880040910028684, 0.37153166411041072, 0.37513083544994319, 0.37201589334520507, 0.36453452797757402, 0.36167426715584011, 0.37335043209557833, 0.35967564918060208, 0.37421966161776871, 0.37368696246687877, 0.38332165182852851, 0.40079881132720846, 0.38619945605974254, 0.39715036436524692, 0.40099064264866607, 0.40236055056688785, 0.41484882497419767, 0.42534484624401908, 0.41220408483868948, 0.41633282311398234, 0.43665565085285074, 0.43698277427345239]

#q = [0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.02, 0.02, 0.029999999999999999, 0.029999999999999999, 0.029999999999999999, 0.040000000000000001, 0.040000000000000001, 0.050000000000000003, 0.050000000000000003, 0.059999999999999998, 0.059999999999999998, 0.070000000000000007, 0.070000000000000007, 0.080000000000000002, 0.080000000000000002, 0.089999999999999997, 0.089999999999999997, 0.10000000000000001, 0.10000000000000001, 0.11, 0.11, 0.12, 0.12, 0.13, 0.13, 0.14000000000000001, 0.14000000000000001, 0.14999999999999999, 0.14999999999999999, 0.16, 0.16]
#u = [8.4406054485615716, 7.1162225853751906, 5.9955672237176385, 5.0473128644743515, 4.2449499405342275, 3.5660451393853143, 2.991614616256379, 2.505593585380792, 2.0943874697636735, 1.7464920685777103, 1.4521721296654075, 1.2031893464641406, 0.99257217958862376, 0.81442107188626689, 0.66374361467445975, 0.5363150597047599, 0.42856027955856058, 0.33745387844791136, 0.26043566251746675, 0.19533910788797335, 0.14033082783855833, 0.093859347841014323, 0.054611757220652965, 0.021477026291198117, -0.0064850359554608755, -0.030070050926729638, -0.049951300231962516, -0.066698537310964187, -0.080793906485186889, -0.092645414209892266, -0.10259832891404204, -0.11094482793941092, -0.11793216111439202, -0.12376955905273122, -0.128634079195039, -0.13267555293148231, -0.13602077202822815, -0.13877703132643437, -0.14103512669673454]
#q_nxt = [0.0, 0.00084406054485615725, 0.0023997433482498337, 0.0045549828740152742, 0.0072149536862281495, 0.010299419492494447, 0.013740489812699276, 0.017480721594529743, 0.02147151273489829, 0.025671742622243206, 0.030046621716445891, 0.03456671802361512, 0.039207133265430759, 0.043946805725205262, 0.048767920292168393, 0.053655409220598971, 0.058596529655000021, 0.063580506117356925, 0.068598227967558628, 0.073641993384012075, 0.078705292711254315, 0.083782625121280416, 0.088869343466090614, 0.093961522986622886, 0.099055850209784274, 0.10414952892935012, 0.10924020064382328, 0.11432587722827325, 0.11940488395899213, 0.12447581129906247, 0.12953747409771185, 0.13458887706346981, 0.13962918554643383, 0.14465770081328641, 0.14967383912423371, 0.1546771140272615, 0.15966712137499617, 0.164643526645528, 0.16960605421292718]




data1 = [100]

for index in range(len(q)):
   temp = [q[index],u[index]]
   data1.append(temp)
   #data.append()
   #zs = q_nxt[index]

#print(data1)




#Plot the data

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
n = 100
#for c, m, zl, zh in [('r', 'o', -50, -25)]:
    
for index in range(len(q)):
   xs = q[index]
   ys = u[index]
   zs = q_nxt[index]
   ax.scatter(xs, ys, zs)#, c=c, marker=m)



ax.set_xlabel('Q')
ax.set_ylabel('U')
ax.set_zlabel('Q_NXT')

#plt.show()


#Format is strange as it dows power bt its everything from first to second in data columns
#change this so that it is two we pass in so q, u 
#write out to a csv file then read back in 
X = data[:, :2]


#with open('test.csv', 'w', newline='') as fp:
 #   a = csv.writer(fp, delimiter=',')
  #  data1 = [
   #         [293, 219],
    #        [54, 13]]
    #a.writerows(data1)

#data = loadtxt('test.csv', delimiter=',')



#print(X)

#Represent data we want to predict
#in out case q_nxt
y = data[:, 2]
#print(y)


#number of training samples
m = y.size

y.shape = (m, 1)

#Scale features and set them to zero mean ??
x, mean_r, std_r = feature_normalize(X)
#print(x)

#x data is between 0-1 more or less
#This just adds ones as a best practice technique so0 indexed
#Add a column of ones to X (interception data)
it = ones(shape=(m, 3))
it[:, 1:3] = x
#print(x)








#Some gradient descent settings
iterations = 100
#origninal alpha 0.01
alpha = 0.1

#Init Theta and Run Gradient Descent
theta = zeros(shape=(3, 1))

#perfect model data

#q = [0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.02, 0.02, 0.029999999999999999, 0.029999999999999999, 0.029999999999999999, 0.040000000000000001, 0.040000000000000001, 0.050000000000000003, 0.050000000000000003, 0.059999999999999998, 0.059999999999999998, 0.070000000000000007, 0.070000000000000007, 0.080000000000000002, 0.080000000000000002, 0.089999999999999997, 0.089999999999999997, 0.10000000000000001, 0.10000000000000001, 0.11, 0.11, 0.12, 0.12, 0.13, 0.13, 0.14000000000000001, 0.14000000000000001, 0.14999999999999999, 0.14999999999999999, 0.16, 0.16]
#u = [8.4406054485615716, 7.1162225853751906, 5.9955672237176385, 5.0473128644743515, 4.2449499405342275, 3.5660451393853143, 2.991614616256379, 2.505593585380792, 2.0943874697636735, 1.7464920685777103, 1.4521721296654075, 1.2031893464641406, 0.99257217958862376, 0.81442107188626689, 0.66374361467445975, 0.5363150597047599, 0.42856027955856058, 0.33745387844791136, 0.26043566251746675, 0.19533910788797335, 0.14033082783855833, 0.093859347841014323, 0.054611757220652965, 0.021477026291198117, -0.0064850359554608755, -0.030070050926729638, -0.049951300231962516, -0.066698537310964187, -0.080793906485186889, -0.092645414209892266, -0.10259832891404204, -0.11094482793941092, -0.11793216111439202, -0.12376955905273122, -0.128634079195039, -0.13267555293148231, -0.13602077202822815, -0.13877703132643437, -0.14103512669673454]
#q_nxt = [0.0, 0.00084406054485615725, 0.0023997433482498337, 0.0045549828740152742, 0.0072149536862281495, 0.010299419492494447, 0.013740489812699276, 0.017480721594529743, 0.02147151273489829, 0.025671742622243206, 0.030046621716445891, 0.03456671802361512, 0.039207133265430759, 0.043946805725205262, 0.048767920292168393, 0.053655409220598971, 0.058596529655000021, 0.063580506117356925, 0.068598227967558628, 0.073641993384012075, 0.078705292711254315, 0.083782625121280416, 0.088869343466090614, 0.093961522986622886, 0.099055850209784274, 0.10414952892935012, 0.10924020064382328, 0.11432587722827325, 0.11940488395899213, 0.12447581129906247, 0.12953747409771185, 0.13458887706346981, 0.13962918554643383, 0.14465770081328641, 0.14967383912423371, 0.1546771140272615, 0.15966712137499617, 0.164643526645528, 0.16960605421292718]






#Printing gradient descent running




theta, J_history = gradient_descent(it, y, theta, alpha, iterations)

#theta is a 3 dimensional array which holds the theta value from gradient descent 
#J history is the history of gradient descent
#print(theta)




#print(theta, J_history)
#plot(arange(iterations), J_history)
#xlabel('Iterations')
#ylabel('Cost Function')
#show()




#How does it predict
#Predict price of a 1650 sq-ft 3 br house

#This will be where the real data goes in with errors
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
n = 100

#100 points which reaches the target
q1 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01, 0.02, 0.02, 0.029999999999999999, 0.029999999999999999, 0.029999999999999999, 0.040000000000000001, 0.040000000000000001, 0.050000000000000003, 0.050000000000000003, 0.059999999999999998, 0.059999999999999998, 0.070000000000000007, 0.070000000000000007, 0.080000000000000002, 0.080000000000000002, 0.089999999999999997, 0.089999999999999997, 0.10000000000000001, 0.10000000000000001, 0.11, 0.11, 0.12, 0.12, 0.13, 0.13, 0.14000000000000001, 0.14000000000000001, 0.14999999999999999, 0.14999999999999999, 0.16, 0.16, 0.17000000000000001, 0.17000000000000001, 0.17999999999999999, 0.17999999999999999, 0.19, 0.19, 0.20000000000000001, 0.20000000000000001, 0.20999999999999999, 0.20999999999999999, 0.22, 0.22, 0.23000000000000001, 0.23000000000000001, 0.23999999999999999, 0.23999999999999999, 0.25, 0.25, 0.26000000000000001, 0.26000000000000001, 0.27000000000000002, 0.27000000000000002, 0.28000000000000003, 0.28000000000000003, 0.28000000000000003, 0.28999999999999998, 0.28999999999999998, 0.29999999999999999, 0.29999999999999999, 0.31, 0.31, 0.32000000000000001, 0.32000000000000001, 0.33000000000000002, 0.33000000000000002, 0.33000000000000002, 0.34000000000000002, 0.34000000000000002, 0.34999999999999998, 0.34999999999999998, 0.35999999999999999, 0.35999999999999999, 0.35999999999999999, 0.37, 0.37, 0.38, 0.38, 0.39000000000000001, 0.39000000000000001, 0.39000000000000001, 0.40000000000000002, 0.40000000000000002, 0.40999999999999998, 0.40999999999999998, 0.41999999999999998, 0.41999999999999998, 0.41999999999999998, 0.42999999999999999, 0.42999999999999999, 0.44]
u1 = [8.4406054485615716, 7.1162225853751906, 5.9955672237176385, 5.0473128644743515, 4.2449499405342275, 3.5660451393853143, 2.991614616256379, 2.505593585380792, 2.0943874697636735, 1.7464920685777103, 1.4521721296654075, 1.2031893464641406, 0.99257217958862376, 0.81442107188626689, 0.66374361467445975, 0.5363150597047599, 0.42856027955856058, 0.33745387844791136, 0.26043566251746675, 0.19533910788797335, 0.14033082783855833, 0.093859347841014323, 0.054611757220652965, 0.021477026291198117, -0.0064850359554608755, -0.030070050926729638, -0.049951300231962516, -0.066698537310964187, -0.080793906485186889, -0.092645414209892266, -0.10259832891404204, -0.11094482793941092, -0.11793216111439202, -0.12376955905273122, -0.128634079195039, -0.13267555293148231, -0.13602077202822815, -0.13877703132643437, -0.14103512669673454, -0.14287189201210826, -0.14435234602212896, -0.14553150911225904, -0.14645594070845205, -0.14716504028212943, -0.14769214830562638, -0.14806547791879143, -0.14830890333750446, -0.14844262703224972, -0.14848374431771019, -0.14844672112802137, -0.14834379832672401, -0.14818533384782642, -0.14798009222741726, -0.14773548961532523, -0.1474578011124639, -0.14715233622685803, -0.14682358735060685, -0.14647535540621448, -0.14611085617286401, -0.14573281026338208, -0.14534351926585232, -0.14494493017728038, -0.14453868992958177, -0.1441261915313585, -0.14370861311465319, -0.14328695097766617, -0.14286204754663093, -0.14243461503812338, -0.14200525548291146, -0.14157447767083117, -0.14114271149011376, -0.14071032006181911, -0.14027761000840488, -0.13984484014333751, -0.13941222882453672, -0.13897996017710904, -0.13854818935923141, -0.13811704701831962, -0.13768664306198555, -0.13725706984914421, -0.13682840489043391, -0.13640071313339744, -0.13597404889627684, -0.13554845750445077, -0.13512397667523482, -0.13470063768974791, -0.13427846638457194, -0.13385748399092934, -0.13343770784481351, -0.13301915198791953, -0.13260182767616988, -0.13218574381003581, -0.13177090729868823, -0.13135732336814465, -0.13094499582203195, -0.13053392726224103, -0.13012411927565273, -0.12971557259214003, -0.12930828721827509]
q_nxt1 = [0.0, 0.00084406054485615725, 0.0023997433482498337, 0.0045549828740152742, 0.0072149536862281495, 0.010299419492494447, 0.013740489812699276, 0.017480721594529743, 0.02147151273489829, 0.025671742622243206, 0.030046621716445891, 0.03456671802361512, 0.039207133265430759, 0.043946805725205262, 0.048767920292168393, 0.053655409220598971, 0.058596529655000021, 0.063580506117356925, 0.068598227967558628, 0.073641993384012075, 0.078705292711254315, 0.083782625121280416, 0.088869343466090614, 0.093961522986622886, 0.099055850209784274, 0.10414952892935012, 0.10924020064382328, 0.11432587722827325, 0.11940488395899213, 0.12447581129906247, 0.12953747409771185, 0.13458887706346981, 0.13962918554643383, 0.14465770081328641, 0.14967383912423371, 0.1546771140272615, 0.15966712137499617, 0.164643526645528, 0.16960605421292718, 0.17455447826765669, 0.17948861513318498, 0.18440831676411107, 0.18931346524412593, 0.19420396813006996, 0.19907975451198576, 0.20394077167907101, 0.20878698229836437, 0.21361836202732398, 0.21843489749358039, 0.22323658458540502, 0.22802342700511682, 0.23279543504499597, 0.23755262455149034, 0.24229501604876197, 0.24702263399707206, 0.25173550616527091, 0.25643366309984705, 0.26111713767568817, 0.26578596471598864, 0.27044018067067183, 0.27507982334432868, 0.27970493166605892, 0.28431554549477145, 0.28891170545449102, 0.29349345279505745, 0.29806082927431243, 0.30261387705846965, 0.30715263863787218, 0.3116771567557709, 0.31618747434812133, 0.32068363449270471, 0.32516568036613908, 0.32963365520756727, 0.33408760228799461, 0.33852756488440761, 0.34295358625793815, 0.34736570963545099, 0.35176397819402788, 0.35614843504790294, 0.36051912323747182, 0.36487608572005575, 0.36921936536215066, 0.37354900493293225, 0.37786504709882418, 0.38216753441896567, 0.38645650934143966, 0.39073201420014464, 0.39499409121221118, 0.39924278247587863, 0.40347812996876159, 0.40770017554644578, 0.41190896094136237, 0.41610452776189794, 0.4202869174917036, 0.42445617148917247, 0.42861233098705914, 0.43275543709221959, 0.43688553078545245, 0.4410026529214261]
q_nxt_new = []

dynamics = Dynamics()
    

for index in range(len(q)):
#implementing dynamics for linear regression as discussed
   q =  np.array([0.,0.,0.,0.]) #x,y,dx,dy
   qf  =  np.array([.5682151,   0.5682151,   0.36304926,  0.36304926]) #x,y,dx,dy
 
   u = compute_ctrl(q0=q, qf=qf, dynamics=dynamics)
   q_nxt = dynamics.compute_nxt_state(q=q , u=u, disturb=False)       
   #print("q next =", q_nxt)

   xs = q1[index]
   ys = u1[index]
   X_nxt = array([1.0,   ((xs - mean_r[0]) / std_r[0]), ((ys - mean_r[1]) / std_r[1])]).dot(theta)
   zs = X_nxt
   q_nxt_new.append(X_nxt)
   print('Predicted next position of x, %f' % (X_nxt), 'Actual is, %f' % (q_nxt1[index]))
   ax.scatter(xs, ys, zs)#, c=c, marker=m)

ax.set_xlabel('Q')
ax.set_ylabel('U')
ax.set_zlabel('Q_NXT')

plt.show()
#print(q_nxt)
  




#Prints the house price
#print('Predicted price of a 1650 sq-ft, 3 br house: %f' % (price))


#Apply Linear regression to this example 
#Multivariate or singular 
#Learn and apply example multivariate


def main():
    start =  np.array([0.,0.,0.,0.]) #x,y,dx,dy
    goal  =  np.array([.5682151,   0.5682151,   0.36304926,  0.36304926]) #x,y,dx,dy

    total_data_points = 100

    time_steps = 100

    '''
    This is a dyanmics object of class Dynamics
    '''
    q  = start.copy()
    qf = goal.copy()

    dynamics = Dynamics()
       

    data = []
    q_array = []
    u_array= []
    q_nxt_array = []

    point_mass_trajectory = np.zeros([4, time_steps])
    point_mass_trajectory[point_mass_trajectory==0.] = np.nan
    point_mass_trajectory[:, 0] = start
    
    error_list = np.asarray([np.nan for _ in range(time_steps)])

    error_list[0] = np.linalg.norm(start-goal)
   
    for t in range(1, time_steps):

        u = compute_ctrl(q0=q, qf=qf, dynamics=dynamics)
        
        '''
        the first is the ideal case where the system is able to compute the
        right control command to take it from start to goal
        '''
        #Faulty with non-linear term
        #q_nxt = dynamics.compute_nxt_state(q=q + [np.random.randn() * 0.01,np.random.randn() * 0.01,np.random.randn() * 0.01,np.random.randn() * 0.01], u=u, disturb=False)
            
        ##Perfect model
        q_nxt = dynamics.compute_nxt_state(q=q , u=u, disturb=True)
         
        
        #q = q_nxt + np.random.randn(4)
        
        
        point_mass_trajectory[:, t] = q_nxt
        error_list[t] = np.linalg.norm(q_nxt-qf)
        q_array.append(round(q[0],2))
        
        print(round(q[0],7))
        print(round(u[0],7))
        print(round(q_nxt[0],7))
        u_array.append(u[0])
        q_nxt_array.append(q_nxt[0])
       

        q = q_nxt

        # if(q[0] >= .5682151 ):
        #     print("You have reached the target")
        #     break

        #Given q and u the method should be able to predict q_nxt 
        linearRegression(q ,u, dynamics)
        


        visualize(start, goal, point_mass_trajectory, error_list)
    
    
    #print(q_array)
    #print(u_array)
    #print(q_nxt_array)
    a = np.asarray(q_array)
    #np.savetxt("foo.csv", a, delimiter=",")
    #np.savetxt('q_new.txt', q_array);
    #np.savetxt('q_new_nxt.txt', q_nxt_array);
    #np.savetxt('u_new.txt', u_array);

   #print q_nxt

    '''
    
    this is the actual case and will show show that the computed control wont take the system to the goal 
    due to the presence of non linear term
    '''

    # q_nxt, dq_nxt = dynamics(q=q, dq=dq, u=u, disturb=True)

    # print "Error in q \t", abs(q_nxt-qf)
    # print "Error in dq \t", abs(dq_nxt-dqf)

    # correction_dynamics = collect_data_and_learn_correction()


    # u = compute_ctrl(q, dq, qf, dqf, dynamics, correction_dynamics)

    # '''
    # this is the corrected case and will show show that the computed control wont take the system to the goal 
    # due to the presence of non linear term
    # '''

    # q_nxt, dq_nxt = dynamics(q=q, dq=dq, u=u, disturb=True)

    # print "Error in q \t", abs(q_nxt-qf)
    # print "Error in dq \t", abs(dq_nxt-dqf)

   #raw_input("Press enter to exit...")


if __name__ == '__main__':
    main()
